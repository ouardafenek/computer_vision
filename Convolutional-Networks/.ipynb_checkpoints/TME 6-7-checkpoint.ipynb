{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Réseaux convolutionnels pour l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.CIFAR10.url = \"http://webia.lip6.fr/~robert/cours/rdfia/cifar-10-python.tar.gz\" # Permet de télécharger CIFAR10 depuis les serveurs UPMC\n",
    "\n",
    "from tme6 import *\n",
    "\n",
    "PRINT_INTERVAL = 50\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe contient la structure du réseau de neurones\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # On défini d'abord les couches de convolution et de pooling comme un\n",
    "        # groupe de couches `self.features`\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, (5, 5), stride=1, padding=2),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
    "            nn.Conv2d(6, 16, (5, 5), stride=1, padding=0),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d((2, 2), stride=2, padding=0),\n",
    "        )\n",
    "        # On défini les couches fully connected comme un groupe de couches\n",
    "        # `self.classifier`\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(400, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, 10)\n",
    "            # Rappel : Le softmax est inclus dans la loss, ne pas le mettre ici\n",
    "        )\n",
    "\n",
    "    # méthode appelée quand on applique le réseau à un batch d'input\n",
    "    def forward(self, input):\n",
    "        bsize = input.size(0) # taille du batch\n",
    "        output = self.features(input) # on calcule la sortie des conv\n",
    "        output = output.view(bsize, -1) # on applati les feature map 2D en un\n",
    "                                        # vecteur 1D pour chaque input\n",
    "        output = self.classifier(output) # on calcule la sortie des fc\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar_dataset(batch_size, path):\n",
    "    \"\"\"\n",
    "    Cette fonction charge le dataset et effectue des transformations sur chaqu\n",
    "    image (listées dans `transform=...`).\n",
    "    \"\"\"\n",
    "    train_dataset = datasets.CIFAR10(path, train=True, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "    val_dataset = datasets.CIFAR10(path, train=False, download=True,\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                        batch_size=batch_size, shuffle=True, pin_memory=CUDA, num_workers=2)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                        batch_size=batch_size, shuffle=False, pin_memory=CUDA, num_workers=2)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(data, model, criterion, optimizer=None):\n",
    "    \"\"\"\n",
    "    Fait une passe (appelée epoch en anglais) sur les données `data` avec le\n",
    "    modèle `model`. Evalue `criterion` comme loss.\n",
    "    Si `optimizer` est fourni, effectue une epoch d'apprentissage en utilisant\n",
    "    l'optimiseur donné, sinon, effectue une epoch d'évaluation (pas de backward)\n",
    "    du modèle.\n",
    "    \"\"\"\n",
    "\n",
    "    # indique si le modele est en mode eval ou train (certaines couches se\n",
    "    # comportent différemment en train et en eval)\n",
    "    model.eval() if optimizer is None else model.train()\n",
    "\n",
    "    # objets pour stocker les moyennes des metriques\n",
    "    avg_loss = AverageMeter()\n",
    "    avg_top1_acc = AverageMeter()\n",
    "    avg_top5_acc = AverageMeter()\n",
    "    avg_batch_time = AverageMeter()\n",
    "    global loss_plot\n",
    "\n",
    "    # on itere sur les batchs du dataset\n",
    "    tic = time.time()\n",
    "    for i, (input, target) in enumerate(data):\n",
    "\n",
    "        if CUDA: # si on fait du GPU, passage en CUDA\n",
    "            input = input.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        # forward\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # backward si on est en \"train\"\n",
    "        if optimizer:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # calcul des metriques\n",
    "        prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "        batch_time = time.time() - tic\n",
    "        tic = time.time()\n",
    "\n",
    "        # mise a jour des moyennes\n",
    "        avg_loss.update(loss.item())\n",
    "        avg_top1_acc.update(prec1.item())\n",
    "        avg_top5_acc.update(prec5.item())\n",
    "        avg_batch_time.update(batch_time)\n",
    "        if optimizer:\n",
    "            loss_plot.update(avg_loss.val)\n",
    "        # affichage des infos\n",
    "        if i % PRINT_INTERVAL == 0:\n",
    "            print('[{0:s} Batch {1:03d}/{2:03d}]\\t'\n",
    "                  'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:5.1f} ({top1.avg:5.1f})\\t'\n",
    "                  'Prec@5 {top5.val:5.1f} ({top5.avg:5.1f})'.format(\n",
    "                   \"EVAL\" if optimizer is None else \"TRAIN\", i, len(data), batch_time=avg_batch_time, loss=avg_loss,\n",
    "                   top1=avg_top1_acc, top5=avg_top5_acc))\n",
    "            if optimizer:\n",
    "                loss_plot.plot()\n",
    "\n",
    "    # Affichage des infos sur l'epoch\n",
    "    print('\\n===============> Total time {batch_time:d}s\\t'\n",
    "          'Avg loss {loss.avg:.4f}\\t'\n",
    "          'Avg Prec@1 {top1.avg:5.2f} %\\t'\n",
    "          'Avg Prec@5 {top5.avg:5.2f} %\\n'.format(\n",
    "           batch_time=int(avg_batch_time.sum), loss=avg_loss,\n",
    "           top1=avg_top1_acc, top5=avg_top5_acc))\n",
    "\n",
    "    return avg_top1_acc, avg_top5_acc, avg_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(params):\n",
    "\n",
    "    # ex de params :\n",
    "    #   {\"batch_size\": 128, \"epochs\": 5, \"lr\": 0.1, \"path\": '/tmp/datasets/mnist'}\n",
    "\n",
    "    # define model, loss, optim\n",
    "    model = ConvNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), params.lr)\n",
    "\n",
    "    if CUDA: # si on fait du GPU, passage en CUDA\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    # On récupère les données\n",
    "    train, test = get_cifar_dataset(params.batch_size, params.path)\n",
    "\n",
    "    # init plots\n",
    "    plot = AccLossPlot()\n",
    "    global loss_plot\n",
    "    loss_plot = TrainLossPlot()\n",
    "\n",
    "    # On itère sur les epochs\n",
    "    for i in range(params.epochs):\n",
    "        print(\"=================\\n=== EPOCH \"+str(i+1)+\" =====\\n=================\\n\")\n",
    "        # Phase de train\n",
    "        top1_acc, avg_top5_acc, loss = epoch(train, model, criterion, optimizer)\n",
    "        # Phase d'evaluation\n",
    "        top1_acc_test, top5_acc_test, loss_test = epoch(test, model, criterion)\n",
    "        # plot\n",
    "        plot.update(loss.avg, loss_test.avg, top1_acc.avg, top1_acc_test.avg)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--path DIR] [--epochs N] [--batch-size N]\n",
      "                             [--lr LR] [--cuda]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /Users/ouardafenek/Library/Jupyter/runtime/kernel-9d945a01-2558-4da6-899c-c206ab57a454.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # Paramètres en ligne de commande\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--path', default='/tmp/datasets/mnist', type=str, metavar='DIR', help='path to dataset')\n",
    "    parser.add_argument('--epochs', default=5, type=int, metavar='N', help='number of total epochs to run')\n",
    "    parser.add_argument('--batch-size', default=128, type=int, metavar='N', help='mini-batch size (default: 128)')\n",
    "    parser.add_argument('--lr', default=0.1, type=float, metavar='LR', help='learning rate')\n",
    "    parser.add_argument('--cuda', dest='cuda', action='store_true', help='activate GPU acceleration')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    if args.cuda:\n",
    "        CUDA = True\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "    main(args)\n",
    "\n",
    "    input(\"done\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
