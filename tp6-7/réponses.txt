
2-  L'avantage qu'apporte les couches de convolution est qu'elles réduisent beaucoup le nombre de paramètres à apprendre par rapport à des couches FC. 

La limite principale du produit de convolution c'est qu'il prend du temps à appliquer, l'apprentissage du réseau serait long. De plus, contrairement à l'algorithme SIFT par exemple, on ne garantit pas l'invariance à la translation et l'orientation en utilisat des CNN sur des images. 

3-  Le pooling permet de réduire les dimensions des couches, celà donc réduit le nombre de paramètres à apprendre à la couche suivante sans une grande perte d'informations, vu que toutes les informations du voisinage seront regroupées dans le cas d'Average Pooling, où l'information la plus importante est gardée dans le cas du Max Pooling. Cette opération rend l'apprentissage du réseau faisable et plus rapide en termes de gain d'espace mémoire et de temps de calcul. 

4- Si la taille de l'image en entrée est supérieure à la taille d'entrainement du CNN, on échantillonne l'image en sous fenêtres et on les passe au CNN, c'est comme si le filtre parcourait l'image. Au final, la taille de la sortie sera variable en fonction de la taille de l'entrée. Contrairement à un Fully Connected qui est sensible à la taille de l'image (il ne peut pas parcourir un vecteur).\\

5-  Il est possible de convertir une couche CNN en une couche FC si nous définissons la taille du noyau (kernel) pour correspondre à la taille de l'entrée. Définir le nombre de filtres revient alors définir le nombre de neurones de sortie dan sune couche FC 

6- 

7- 

8-  Si on veut garder les mêmes dimensions de l'entrée, on prend $s=1$ et $p=\frac{k-1}{2}$.

9-  Pour réduire les dimensions spatiales d'un facteur de 2 avec le MaxPooling, on choisit un $p=2$ et un $s=2$.

10- La taille de la sortie et le nombre de poids à apprendre pour chaque couche: \\

On a l'entrée qui est de dimensions: 32x32x3 \\

$conv1:$ 32 convolutions 5×5, stride=1, padding=2: La sortie sera de dimensions : 32x32x32, le nombre de paramètres à apprendre: 32x(3x5x5+1)=2432\\

$pool1:$ stride=2, padding=0: La sortie sera de dimensions: 16x16x32, aucun paramètre à apprendre. \\

$conv2:$ 64 convolutions 5×5, stride=1, padding=0: La sortie sera de dimensions : 12x12x64, le nombre de paramètres à apprendre: 64x(32x5x5+1)=56064\\

$pool2:$ stride=2, padding=0: La sortie sera de dimensions: 6x6x64, aucun paramètre à apprendre. \\

$conv2:$ 64 convolutions 5×5, stride=1, padding=0: La sortie sera de dimensions : 2x2x64, le nombre de paramètres à apprendre: 64x(64x5x5+1)=102464 \\

$pool3:$ stride=2, padding=0: La sortie sera de dimensions: 1x1x64, aucun paramètre à apprendre. \\

$fc4:$ Linear(64, 1000) : La sortie sera de dimension 1x1000, le nombre de paramètres à apprendre 64x1000+1000=65000. \\

$fc5:$ Linear(1000,10) : La sortie sera de dimensions 1x10, le nombre de paramètres à apprendre 1000x10+10=10010. \\ 

On voit que les couches de convolutions ont beaucoup moins de paramètres à apprendre par rapport aux couches fully-connected, et l'application du max-pooling réduit encore les dimensions des sorties, ce qui réduit le nombre de paramètres à apprendre à la couche suivante. \\ 
On applique quand même des fc pour adapter la sortie au problème de classification exprimé.\\

11- Celà fait en total   235970\\ 
Le nombre d'exemples de la base de données CIFAR-10 est relativement petit par rapport à ce nombre de paramètres. \\

20- Généralement on a pas accès au données de test, donc on a interet à utiliser les mêmes paramètres pour tous. De plus l'ensemble d'apprentissage est assez grand, la moyenne calaculée sur ces données serait proche de la moyenne des données de test. Un autre interêt serait de ne pas perdre de généralité, ne pas recalculer la moyenne à chaque fois que l'ensemble de test est changé. \\ 

21- La ZCA est une méthode de normalisation qui pondère les pixels, par exemple, dans une image, les pixels des alentours ne sont pas en général ce qui contiennent l'information pertinente, ils auront donc des poids faibles, contrairement à ceux du milieu.\\ Cependant son utilisation n'est pas très pratique, vu qu'elle est trop lente. \\ 

22- 

23- Cette approche peut coduire à une confusion pour certaines images telles que les images de chiffres, si on applique une symétrie horizontale pour un "6" par exemple, on créera une confusion avec "9". C'est la même confusion qui peut être créée en appliquant une symétrie verticale sur le chiffre "2", on aura un "5". 

24- L'augmentation des données est utile dans une certaine mesure, mais il est toujours préférable d'avoir plus de données. Le réseau s'entrainera, mais lorsqu'il est testé sur un jeu de données inédit, il ne fonctionnera pas bien, on a certes eu des données mais elles ne sont pas de qualité. De plus des fois celà crée une confusion (tel que le cas décris précédement), ou dan sun autre cas ou la couleur serait deterministe, par exemple, les femmes noires ont 42% plus de risques de mourir du cancer de sein en raison d'un large éventail de facteurs, l'application des changements de couleurs pour augmenter les données ne permetterait pas de détecter celà. \\

25- Autres types de data augmentation: Rotation, changement de couleurs, changement d'échelle (zoom), symétrie verticale ... 

26- 

27- Le fait de prendre en considération la direction du gradient fournie au batch précédent ++++++++++++++++++++++++++++++++++++++++++++
Le fait de faire diminuer le pas d'apprentissage après un certain nombre d'époques permet de diminuer la sévérité de la modification des paramètres en avançant dans l'apprentissage, au début, les paramètres sont aléatoires donc on aimerait apporter de grands changements, mais au fur et à mesure de l'apprentissage ils deviennet plus stables, et si un exemple différent intervient on voudrait pas qu'il déstabilise nos paramètres.\\  