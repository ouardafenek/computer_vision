{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torch.nn as nn \n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pickle, PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.models.vgg.model_urls[\"vgg16\"] = \"http://webia.lip6.fr/~robert/cours/rdfia/vgg16-397923af.pth\"\n",
    "os.environ[\"TORCH_MODEL_ZOO\"] = \"/tmp/torch\"\n",
    "PRINT_INTERVAL = 50\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(batch_size, path):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.0229,0.224,0.225]\n",
    "    # Cette fonction permet de recopier 3 fois une image qui\n",
    "    # ne serait que sur 1 channel (donc image niveau de gris)\n",
    "    # pour la \"transformer\" en image RGB. Utilisez la avec\n",
    "    # transform.Lambda\n",
    "    def duplicateChannel(img):\n",
    "        img = img.convert('L')\n",
    "        np_img = np.array(img, dtype=np.uint8)\n",
    "        np_img = np.dstack([np_img, np_img, np_img])\n",
    "        img = Image.fromarray(np_img, 'RGB')\n",
    "        return img\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(path+'/train',\n",
    "        transform=transforms.Compose([ # TODO Pré-traitement à faire\n",
    "            transforms.Lambda(lambda img : duplicateChannel(img)),\n",
    "            transforms.Lambda(lambda img : img.resize((224, 224), PIL.Image.BILINEAR)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean,std)\n",
    "        ]))\n",
    "    val_dataset = datasets.ImageFolder(path+'/test',\n",
    "        transform=transforms.Compose([ # TODO Pré-traitement à faire\n",
    "            transforms.Lambda(lambda img : duplicateChannel(img)),\n",
    "            transforms.Lambda(lambda img : img.resize((224, 224), PIL.Image.BILINEAR)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean,std)\n",
    "        ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                        batch_size=batch_size, shuffle=False, pin_memory=CUDA, num_workers=2)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                        batch_size=batch_size, shuffle=False, pin_memory=CUDA, num_workers=2)\n",
    "\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/hub.py:463: UserWarning: TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead\n",
      "  warnings.warn('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')\n"
     ]
    }
   ],
   "source": [
    "import os, torchvision\n",
    "torchvision.models.vgg.model_urls[\"vgg16\"] =\"http://webia.lip6.fr/~robert/cours/rdfia/vgg16-397923af.pth\"\n",
    "os.environ[\"TORCH_HOME\"] = \"/tmp/torch\"\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(data, model):\n",
    "    # TODO init features matrices\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, (input, target) in enumerate(data):\n",
    "        #if i % PRINT_INTERVAL == 0:\n",
    "        print('Batch {0:03d}/{1:03d}'.format(i, len(data)))\n",
    "        if CUDA:\n",
    "            input = input.cuda()\n",
    "        # TODO Feature extraction à faire\n",
    "        X.append(model.forward(input))\n",
    "        y.append(target)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16relu7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16relu7, self).__init__()\n",
    "        # recopier toute la partie convolutionnelle\n",
    "        self.features = nn.Sequential( *list(vgg16.features.children()))\n",
    "        # garder une partie du classifieur, -2 pour s'arrêter à relu7\n",
    "        self.classifier = nn.Sequential(*list(vgg16.classifier.children())[:-2])\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    path = '15SceneData' \n",
    "    batch_size = 8 \n",
    "    \n",
    "    print('Instanciation de VGG16')\n",
    "    vgg16 = models.vgg16(pretrained=True)\n",
    "\n",
    "    print('Instanciation de VGG16relu7')\n",
    "    model = VGG16relu7() # TODO À remplacer par un reseau tronché pour faire de la feature extraction\n",
    "\n",
    "    model.eval()\n",
    "    if CUDA: # si on fait du GPU, passage en CUDA\n",
    "        model = model.cuda()\n",
    "\n",
    "    # On récupère les données\n",
    "    print('Récupération des données')\n",
    "    train, test = get_dataset(batch_size, path)\n",
    "\n",
    "    # Extraction des features\n",
    "    print('Feature extraction')\n",
    "    X_train, y_train = extract_features(train, model)\n",
    "    X_test, y_test = extract_features(test, model)\n",
    "\n",
    "    # TODO Apprentissage et évaluation des SVM à faire\n",
    "    print('Apprentissage des SVM')\n",
    "    #accuracy = ...\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instanciation de VGG16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/hub.py:463: UserWarning: TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead\n",
      "  warnings.warn('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instanciation de VGG16relu7\n",
      "Récupération des données\n",
      "Feature extraction\n",
      "Batch 000/188\n",
      "Batch 001/188\n",
      "Batch 002/188\n",
      "Batch 003/188\n",
      "Batch 004/188\n",
      "Batch 005/188\n",
      "Batch 006/188\n",
      "Batch 007/188\n",
      "Batch 008/188\n",
      "Batch 009/188\n",
      "Batch 010/188\n",
      "Batch 011/188\n",
      "Batch 012/188\n",
      "Batch 013/188\n",
      "Batch 014/188\n",
      "Batch 015/188\n",
      "Batch 016/188\n",
      "Batch 017/188\n",
      "Batch 018/188\n",
      "Batch 019/188\n",
      "Batch 020/188\n",
      "Batch 021/188\n",
      "Batch 022/188\n",
      "Batch 023/188\n",
      "Batch 024/188\n",
      "Batch 025/188\n",
      "Batch 026/188\n",
      "Batch 027/188\n",
      "Batch 028/188\n",
      "Batch 029/188\n",
      "Batch 030/188\n",
      "Batch 031/188\n",
      "Batch 032/188\n",
      "Batch 033/188\n",
      "Batch 034/188\n",
      "Batch 035/188\n",
      "Batch 036/188\n",
      "Batch 037/188\n",
      "Batch 038/188\n",
      "Batch 039/188\n",
      "Batch 040/188\n",
      "Batch 041/188\n",
      "Batch 042/188\n",
      "Batch 043/188\n",
      "Batch 044/188\n",
      "Batch 045/188\n",
      "Batch 046/188\n",
      "Batch 047/188\n",
      "Batch 048/188\n",
      "Batch 049/188\n",
      "Batch 050/188\n",
      "Batch 051/188\n",
      "Batch 052/188\n",
      "Batch 053/188\n",
      "Batch 054/188\n",
      "Batch 055/188\n",
      "Batch 056/188\n",
      "Batch 057/188\n",
      "Batch 058/188\n",
      "Batch 059/188\n",
      "Batch 060/188\n",
      "Batch 061/188\n",
      "Batch 062/188\n",
      "Batch 063/188\n",
      "Batch 064/188\n",
      "Batch 065/188\n",
      "Batch 066/188\n",
      "Batch 067/188\n",
      "Batch 068/188\n",
      "Batch 069/188\n",
      "Batch 070/188\n",
      "Batch 071/188\n",
      "Batch 072/188\n",
      "Batch 073/188\n",
      "Batch 074/188\n",
      "Batch 075/188\n",
      "Batch 076/188\n",
      "Batch 077/188\n",
      "Batch 078/188\n",
      "Batch 079/188\n",
      "Batch 080/188\n",
      "Batch 081/188\n",
      "Batch 082/188\n",
      "Batch 083/188\n",
      "Batch 084/188\n",
      "Batch 085/188\n",
      "Batch 086/188\n",
      "Batch 087/188\n",
      "Batch 088/188\n",
      "Batch 089/188\n",
      "Batch 090/188\n",
      "Batch 091/188\n",
      "Batch 092/188\n",
      "Batch 093/188\n",
      "Batch 094/188\n",
      "Batch 095/188\n",
      "Batch 096/188\n",
      "Batch 097/188\n",
      "Batch 098/188\n",
      "Batch 099/188\n",
      "Batch 100/188\n",
      "Batch 101/188\n",
      "Batch 102/188\n",
      "Batch 103/188\n"
     ]
    }
   ],
   "source": [
    "CUDA = False \n",
    "X_train, y_train, X_test, y_test = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
